#!/usr/bin/env python3
"""
Exemplo de integraÃ§Ã£o com Ollama e Llama 3.1
Este script demonstra como usar a LLM para anÃ¡lise de dados de acidentes
"""

import ollama
import pandas as pd
import json

def load_sample_data():
    """Carrega dados de exemplo para demonstraÃ§Ã£o"""
    try:
        df = pd.read_csv('/home/ubuntu/upload/datatran2023.csv', sep=';', encoding='latin1')
        return df.head(100)  # Amostra pequena para exemplo
    except Exception as e:
        print(f"Erro ao carregar dados: {e}")
        return pd.DataFrame()

def analyze_with_llm(data_summary, question):
    """
    Analisa dados usando Llama 3.1 via Ollama
    
    Args:
        data_summary (str): Resumo dos dados
        question (str): Pergunta a ser respondida
    
    Returns:
        str: Resposta da LLM
    """
    try:
        prompt = f"""
        VocÃª Ã© um especialista em anÃ¡lise de dados de trÃ¢nsito e seguranÃ§a viÃ¡ria.
        
        Dados disponÃ­veis:
        {data_summary}
        
        Pergunta: {question}
        
        Por favor, forneÃ§a uma anÃ¡lise detalhada e insights baseados nos dados apresentados.
        Seja especÃ­fico e mencione padrÃµes, tendÃªncias e recomendaÃ§Ãµes prÃ¡ticas.
        """
        
        response = ollama.chat(
            model='llama3.1',
            messages=[
                {
                    'role': 'user',
                    'content': prompt
                }
            ]
        )
        
        return response['message']['content']
        
    except Exception as e:
        return f"Erro ao conectar com Ollama: {e}"

def generate_data_summary(df):
    """Gera um resumo dos dados para a LLM"""
    if df.empty:
        return "Nenhum dado disponÃ­vel"
    
    summary = {
        "total_registros": len(df),
        "periodo": "2023 (amostra)",
        "principais_causas": df['causa_acidente'].value_counts().head(5).to_dict() if 'causa_acidente' in df.columns else {},
        "ufs_com_mais_acidentes": df['uf'].value_counts().head(5).to_dict() if 'uf' in df.columns else {},
        "tipos_acidente": df['tipo_acidente'].value_counts().to_dict() if 'tipo_acidente' in df.columns else {},
        "condicoes_meteorologicas": df['condicao_metereologica'].value_counts().to_dict() if 'condicao_metereologica' in df.columns else {}
    }
    
    return json.dumps(summary, indent=2, ensure_ascii=False)

def main():
    """FunÃ§Ã£o principal do exemplo"""
    print("ğŸš— Exemplo de AnÃ¡lise de Acidentes com Llama 3.1")
    print("=" * 50)
    
    # Carregar dados
    print("ğŸ“Š Carregando dados...")
    df = load_sample_data()
    
    if df.empty:
        print("âŒ NÃ£o foi possÃ­vel carregar os dados.")
        return
    
    # Gerar resumo dos dados
    data_summary = generate_data_summary(df)
    print(f"âœ… Dados carregados: {len(df)} registros")
    print("\nğŸ“‹ Resumo dos dados:")
    print(data_summary)
    
    # Perguntas de exemplo
    questions = [
        "Quais sÃ£o os principais fatores de risco para acidentes de trÃ¢nsito baseado nos dados?",
        "Em que horÃ¡rios do dia ocorrem mais acidentes e por quÃª?",
        "Quais condiÃ§Ãµes meteorolÃ³gicas sÃ£o mais perigosas para dirigir?",
        "Que recomendaÃ§Ãµes vocÃª daria para reduzir acidentes de trÃ¢nsito?",
        "Existe algum padrÃ£o nos tipos de acidentes por estado?"
    ]
    
    print("\nğŸ¤– AnÃ¡lises com Llama 3.1:")
    print("=" * 50)
    
    for i, question in enumerate(questions, 1):
        print(f"\n{i}. {question}")
        print("-" * 60)
        
        response = analyze_with_llm(data_summary, question)
        print(response)
        print("\n" + "="*60)

def interactive_mode():
    """Modo interativo para perguntas personalizadas"""
    print("\nğŸ”„ Modo Interativo")
    print("Digite suas perguntas (ou 'sair' para terminar):")
    
    df = load_sample_data()
    if df.empty:
        print("âŒ NÃ£o foi possÃ­vel carregar os dados.")
        return
    
    data_summary = generate_data_summary(df)
    
    while True:
        question = input("\nâ“ Sua pergunta: ").strip()
        
        if question.lower() in ['sair', 'exit', 'quit']:
            print("ğŸ‘‹ AtÃ© logo!")
            break
        
        if not question:
            continue
        
        print("\nğŸ¤– Analisando...")
        response = analyze_with_llm(data_summary, question)
        print(f"\nğŸ’¡ Resposta:\n{response}")

if __name__ == "__main__":
    try:
        # Verificar se Ollama estÃ¡ disponÃ­vel
        ollama.list()
        print("âœ… Ollama conectado com sucesso!")
        
        # Executar anÃ¡lises automÃ¡ticas
        main()
        
        # Modo interativo
        interactive_mode()
        
    except Exception as e:
        print(f"âŒ Erro ao conectar com Ollama: {e}")
        print("\nğŸ“ InstruÃ§Ãµes para configurar Ollama:")
        print("1. Instalar: curl -fsSL https://ollama.ai/install.sh | sh")
        print("2. Iniciar: ollama serve")
        print("3. Baixar modelo: ollama pull llama3.1")

